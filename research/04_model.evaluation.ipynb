{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e9bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754a01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\forgery detection model\\\\image_forgery_detection_model\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9539eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307125f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\forgery detection model\\\\image_forgery_detection_model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab47d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as sharonjolly\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as sharonjolly\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"sharonjolly/image_forgery_detection_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"sharonjolly/image_forgery_detection_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository sharonjolly/image_forgery_detection_model initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository sharonjolly/image_forgery_detection_model initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='sharonjolly', repo_name='image_forgery_detection_model', mlflow=True)\n",
    "\n",
    "# import mlflow\n",
    "# with mlflow.start_run():\n",
    "#   mlflow.log_param('parameter name', 'value')\n",
    "#   mlflow.log_metric('metric name', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdaa36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    model_path: Path\n",
    "    model: str\n",
    "    load_data: Path\n",
    "    mlflow_uri: str\n",
    "    params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1f6d63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 10:28:54,638: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/repos/sharonjolly/image_forgery_detection_model \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"sharonjolly/image_forgery_detection_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"sharonjolly/image_forgery_detection_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 10:28:54,650: INFO: helpers: Initialized MLflow to track repo \"sharonjolly/image_forgery_detection_model\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository sharonjolly/image_forgery_detection_model initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository sharonjolly/image_forgery_detection_model initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 10:28:54,655: INFO: helpers: Repository sharonjolly/image_forgery_detection_model initialized!]\n"
     ]
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='sharonjolly',repo_name='image_forgery_detection_model',mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "771bba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b092284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "    \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        \n",
    "    def get_model_evaluation_config(self) -> EvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.trainer  # <-- fixed\n",
    "\n",
    "        model_evaluation_config = EvaluationConfig(\n",
    "            model_path=config.model_path,\n",
    "            model=config.model,\n",
    "            load_data=config.load_data,\n",
    "            mlflow_uri=config.mlflow_uri,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53b05405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cnnClassifier import logger\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from cnnClassifier.utils.common import save_json\n",
    "import os\n",
    "import tempfile\n",
    "from mlflow.exceptions import RestException\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d65985fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.score = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads test data from joblib files specified in config.\"\"\"\n",
    "        logger.info(f\"Fetching test dataset from: {self.config.load_data}\")\n",
    "        try:\n",
    "            x_path = Path(self.config.load_data) / 'X_90.joblib'\n",
    "            y_path = Path(self.config.load_data) / 'y.joblib'\n",
    "            X = joblib.load(x_path)\n",
    "            y = joblib.load(y_path)\n",
    "            logger.info(f\"Data successfully loaded — X: {X.shape}, y: {y.shape}\")\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unable to load dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def split_data(self, X, y):\n",
    "        \"\"\"Splits data into training and testing sets.\"\"\"\n",
    "        logger.info(\"Separating dataset into training and testing subsets\")\n",
    "        try:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            logger.info(f\"Test subset prepared — X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "            return X_test, y_test\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed during dataset splitting: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_data(self, X_test, y_test):\n",
    "        \"\"\"Reshapes test data for CNN input.\"\"\"\n",
    "        logger.info(\"Reshaping test data for model compatibility\")\n",
    "        try:\n",
    "            X_test = X_test.reshape(X_test.shape[0], 128, 128, 3)\n",
    "            y_test = y_test.reshape(y_test.shape[0], 2)\n",
    "            logger.info(f\"Data reshaped — X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "            self.X_test, self.y_test = X_test, y_test\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while reshaping data: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        \"\"\"Loads the trained model.\"\"\"\n",
    "        logger.info(f\"Retrieving trained model from path: {path}\")\n",
    "        try:\n",
    "            return tf.keras.models.load_model(path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_test_generator(self):\n",
    "        \"\"\"Returns a Sequence generator for test data.\"\"\"\n",
    "        class TestGenerator(Sequence):\n",
    "            def __init__(self, X, y, batch_size, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.X = X\n",
    "                self.y = y\n",
    "                self.batch_size = batch_size\n",
    "                self.indexes = np.arange(len(self.X))\n",
    "\n",
    "            def __len__(self):\n",
    "                return int(np.floor(len(self.X) / self.batch_size))\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "                X = [self.X[k] for k in indexes]\n",
    "                y = [self.y[k] for k in indexes]\n",
    "                return np.array(X), np.array(y)\n",
    "\n",
    "        return TestGenerator(self.X_test, self.y_test, self.config.params['batch_size'])\n",
    "\n",
    "    def evaluation(self):\n",
    "        \"\"\"Evaluates the model and saves scores.\"\"\"\n",
    "        logger.info(\"Starting model evaluation process\")\n",
    "        try:\n",
    "            X, y = self.load_data()\n",
    "            X_test, y_test = self.split_data(X, y)\n",
    "            self.preprocess_data(X_test, y_test)\n",
    "\n",
    "            model_path = Path(self.config.model_path) / self.config.model\n",
    "            self.model = self.load_model(model_path)\n",
    "\n",
    "            test_generator = self.get_test_generator()\n",
    "\n",
    "            logger.info(\"Evaluating model performance on test dataset\")\n",
    "            self.score = self.model.evaluate(\n",
    "                test_generator,\n",
    "                batch_size=self.config.params['batch_size'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            logger.info(f\"Evaluation completed — Scores: {self.score}\")\n",
    "\n",
    "            self.save_score()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Evaluation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def save_score(self):\n",
    "        \"\"\"Saves evaluation scores to a JSON file.\"\"\"\n",
    "        logger.info(\"Saving evaluation results to JSON file\")\n",
    "        try:\n",
    "            f1_score = self.score.get('f1_score', 0.0)\n",
    "            if isinstance(f1_score, tf.Tensor):\n",
    "                f1_score = np.mean(f1_score.numpy())\n",
    "            elif isinstance(f1_score, np.ndarray):\n",
    "                f1_score = np.mean(f1_score)\n",
    "\n",
    "            scores = {\n",
    "                \"loss\": float(self.score.get('loss', 0.0)),\n",
    "                \"accuracy\": float(self.score.get('accuracy', 0.0)),\n",
    "                \"precision\": float(self.score.get('precision', 0.0)),\n",
    "                \"recall\": float(self.score.get('recall', 0.0)),\n",
    "                \"f1_score\": float(f1_score)\n",
    "            }\n",
    "            save_json(path=Path(\"scores.json\"), data=scores)\n",
    "            logger.info(\"Results successfully saved to scores.json\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save results: {e}\")\n",
    "            raise\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"Logs parameters, metrics, and model to MLflow.\"\"\"\n",
    "        logger.info(\"Initiating MLflow logging sequence\")\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "            tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "            logger.info(f\"MLflow tracking URI set to: {self.config.mlflow_uri}\")\n",
    "\n",
    "            with mlflow.start_run():\n",
    "                mlflow.log_params(self.config.params)\n",
    "\n",
    "                f1_score = self.score.get('f1_score', 0.0)\n",
    "                if isinstance(f1_score, tf.Tensor):\n",
    "                    f1_score = np.mean(f1_score.numpy())\n",
    "                elif isinstance(f1_score, np.ndarray):\n",
    "                    f1_score = np.mean(f1_score)\n",
    "\n",
    "                mlflow.log_metrics({\n",
    "                    \"loss\": float(self.score.get('loss', 0.0)),\n",
    "                    \"accuracy\": float(self.score.get('accuracy', 0.0)),\n",
    "                    \"precision\": float(self.score.get('precision', 0.0)),\n",
    "                    \"recall\": float(self.score.get('recall', 0.0)),\n",
    "                    \"f1_score\": float(f1_score)\n",
    "                })\n",
    "\n",
    "                with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                    temp_model_path = os.path.join(tmpdirname, \"model.keras\")\n",
    "                    logger.info(f\"Saving model temporarily at: {temp_model_path}\")\n",
    "                    self.model.save(temp_model_path)\n",
    "                    if not os.path.exists(temp_model_path):\n",
    "                        raise FileNotFoundError(f\"Model file not found at {temp_model_path}\")\n",
    "                    logger.info(f\"Model saved — size: {os.path.getsize(temp_model_path)} bytes\")\n",
    "\n",
    "                    logger.info(\"Uploading model artifact to MLflow\")\n",
    "                    mlflow.log_artifact(temp_model_path, artifact_path=\"model\")\n",
    "                    logger.info(\"Model artifact uploaded successfully\")\n",
    "\n",
    "                if tracking_url_type_store != \"file\":\n",
    "                    logger.info(\"Attempting model registration in MLflow registry\")\n",
    "                    client = MlflowClient()\n",
    "                    run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "                    try:\n",
    "                        client.get_registered_model(\"image_forgery_detection_model\")\n",
    "                    except RestException:\n",
    "                        logger.info(\"Model not found in registry — creating new entry\")\n",
    "                        client.create_registered_model(\"image_forgery_detection_model\")\n",
    "\n",
    "                    try:\n",
    "                        source = mlflow.get_artifact_uri(\"model\")\n",
    "                        logger.info(f\"Model source URI: {source}\")\n",
    "                        result = client.create_model_version(\n",
    "                            name=\"image_forgery_detection_model\",\n",
    "                            source=source,\n",
    "                            run_id=run_id\n",
    "                        )\n",
    "                        logger.info(f\"Model registered successfully — version {result.version}\")\n",
    "                    except RestException as e:\n",
    "                        logger.error(f\"MLflow registry error: {e.__class__.__name__} - {str(e)}\")\n",
    "                        if hasattr(e, \"message\"):\n",
    "                            logger.error(f\"Message: {e.message}\")\n",
    "                        if hasattr(e, \"error_code\"):\n",
    "                            logger.error(f\"Error code: {e.error_code}\")\n",
    "                        raise\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Unexpected model registry error: {e}\")\n",
    "                        raise\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"MLflow logging failed: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aca2d7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 14:03:14,197: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-08-11 14:03:14,206: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-11 14:03:14,211: INFO: 986613851: Starting model evaluation process]\n",
      "[2025-08-11 14:03:14,212: INFO: 986613851: Fetching test dataset from: artifacts/data_preprocessing/pickle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 14:03:16,920: INFO: 986613851: Data successfully loaded — X: (9501, 49152), y: (9501, 2)]\n",
      "[2025-08-11 14:03:16,922: INFO: 986613851: Separating dataset into training and testing subsets]\n",
      "[2025-08-11 14:03:17,623: INFO: 986613851: Test subset prepared — X_test: (1901, 49152), y_test: (1901, 2)]\n",
      "[2025-08-11 14:03:17,793: INFO: 986613851: Reshaping test data for model compatibility]\n",
      "[2025-08-11 14:03:17,795: INFO: 986613851: Data reshaped — X_test: (1901, 128, 128, 3), y_test: (1901, 2)]\n",
      "[2025-08-11 14:03:17,796: INFO: 986613851: Retrieving trained model from path: artifacts\\model_trainer\\model\\model.keras]\n",
      "[2025-08-11 14:03:18,229: INFO: 986613851: Evaluating model performance on test dataset]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.9100 - f1_score: 0.8701 - loss: 0.2331 - precision: 0.9100 - recall: 0.9100\n",
      "[2025-08-11 14:03:22,917: INFO: 986613851: Evaluation completed — Scores: {'accuracy': 0.9099576473236084, 'f1_score': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.94205856, 0.7980997 ], dtype=float32)>, 'loss': 0.233064666390419, 'precision': 0.9099576473236084, 'recall': 0.9099576473236084}]\n",
      "[2025-08-11 14:03:22,918: INFO: 986613851: Saving evaluation results to JSON file]\n",
      "[2025-08-11 14:03:22,922: INFO: common: json file saved at: scores.json]\n",
      "[2025-08-11 14:03:22,924: INFO: 986613851: Results successfully saved to scores.json]\n",
      "[2025-08-11 14:03:23,053: INFO: 986613851: Initiating MLflow logging sequence]\n",
      "[2025-08-11 14:03:23,055: INFO: 986613851: MLflow tracking URI set to: https://dagshub.com/sharonjolly/image_forgery_detection_model.mlflow]\n",
      "[2025-08-11 14:03:24,703: INFO: 986613851: Saving model temporarily at: C:\\Users\\HP\\AppData\\Local\\Temp\\tmpsv5fetr6\\model.keras]\n",
      "[2025-08-11 14:03:24,986: INFO: 986613851: Model saved — size: 39256344 bytes]\n",
      "[2025-08-11 14:03:24,989: INFO: 986613851: Uploading model artifact to MLflow]\n",
      "[2025-08-11 14:04:04,637: INFO: 986613851: Model artifact uploaded successfully]\n",
      "[2025-08-11 14:04:04,648: INFO: 986613851: Attempting model registration in MLflow registry]\n",
      "[2025-08-11 14:04:05,444: INFO: 986613851: Model source URI: mlflow-artifacts:/6727ec6f06f444748784c38f1fd3c07f/45e077f59e9a47759d7a0021f07aba2c/artifacts/model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/11 14:04:05 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: image_forgery_detection_model, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 14:04:05,799: INFO: 986613851: Model registered successfully — version 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "        config = ConfigurationManager()\n",
    "        eval_config = config.get_model_evaluation_config()\n",
    "        evaluation = ModelEvaluation(eval_config)\n",
    "        evaluation.evaluation()\n",
    "        evaluation.log_into_mlflow()\n",
    "except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e8445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".imagemodel (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
